---
title: "CPICS Metadata"
# output: html_notebook
output: none
---


# Load data files
```{r}
library(tidyverse)

# specify the directory where the files are located
dir_path <- "~/Library/CloudStorage/GoogleDrive-enriquemontes01@gmail.com/My Drive/GDrive/OCED_AOML/WS_cruises/plankton_imaging/CPICS/TS.Master_selection"

# obtain a list of file names in the directory
file_names <- list.files(path = dir_path, pattern = ".txt", full.names = TRUE)

# loop over each file and import the tables (use this for DATES)
for (file in file_names) {
  table_name <- gsub(".txt", "", basename(file)) # get the name of the table from the file name
  assign(table_name, read.table(file = file, header = FALSE, sep = "\t") %>%
           mutate(date = as.POSIXct(substr(V1, start = 24, stop = 36), format="%Y%m%d_%H%M", tz="UTC")))
}

```


# # This section detects short transit times between stations 
```{r}
library(lubridate)

dir_path2 <- "~/Library/CloudStorage/GoogleDrive-enriquemontes01@gmail.com/My Drive/GDrive/OCED_AOML/WS_cruises/plankton_imaging/CPICS/ws_cruise_ctd"

file_name <- list.files(path = dir_path2, pattern = "ctd_meta_v4.csv", full.names = TRUE)
ctd_meta <- read.csv(file_name, fill = TRUE)

# USE WITH ctd_meta_v4.csv
dt_list <- as.POSIXct(paste(ctd_meta$year,
                            sprintf("%02d", ctd_meta$month),
                            sprintf("%02d", ctd_meta$day),
                            ctd_meta$time_gmt),
                      format = "%Y%m%d %I:%M:%S %p",
                      tz = "UTC")

# Calculate time differences in seconds between consecutive dt_list objects
time_differences <- as.numeric(difftime(dt_list[-1], dt_list[-length(dt_list)], units = "secs"))

# Convert time differences from seconds to minutes
time_differences_mins <- time_differences / 60

# Create a data frame showing the original times and their differences in minutes
time_diff_df <- data.frame(
  start_time = dt_list[-length(dt_list)],
  end_time = dt_list[-1],
  time_difference_mins = time_differences_mins
)

# find CTD time stamps of consecutive stations within less than 20 min. This will identify CTD casts that are close to each other
short_t_idx <- which(time_diff_df$time_difference_mins < 20)
short_timestamps <- dt_list[short_t_idx]

# Print or return the list of missing filenames
print("Time stamps of stations occupied within less that 20 min apart:")
short_timestamps
```

# # Match file name using DATE values
```{r}
# Create empty data frame to store results
conc_occ_final <- data.frame(date = character(), count = numeric())

# List of class objects to be processed
class_names <- c("Acantharea", "Centric", "Ceratium", "Chaetoceros", "Chaetognaths", 
                 "Chain2", "Chain3", "Ostracods", "Copepods", "Decapods", "Echinoderms", 
                 "Guinardia", "Jellies", "Larvaceans", "Neocalyptrella", "Noctiluca", 
                 "pellets", "Polychaets", "Pteropods", "Tricho")

# time buffer before and after CTD time in seconds so that CPICS records are matched to CTD times.
start <- 10 * 60 
stop <- 10 * 60 

# Define the date limit
date_limit <- as.POSIXct("2024-11-30", tz = "UTC")

# Iterate over dt_list intervals
for (i in 1:length(dt_list)) {
  
  # Initialize a list to store counts for the current interval
  counts_list <- list(date = dt_list[i])
  
  # Iterate over each class object and perform subsetting
  for (class_name in class_names) {
    class_data <- get(paste0("class.", class_name))  # Dynamically get the class data frame
    
    # Filter out rows with dates after the date limit
    class_data <- subset(class_data, date <= date_limit)
    
    if (i < length(dt_list)) {
      # Subsetting for all intervals except the last one
      subset_data <- subset(class_data, date >= dt_list[i]-start & date < dt_list[i+1]-stop)
    } else {
      # Subsetting for the last interval: capture all data greater than or equal to the last dt_list
      subset_data <- subset(class_data, date >= dt_list[i]-start)
    }
    
    counts_list[[class_name]] <- nrow(subset_data)
  }
  
  # Convert counts_list to a data frame and bind it to the result
  result <- as.data.frame(counts_list)
  conc_occ_final <- rbind(conc_occ_final, result)
} 

# Combine with ctd_meta
taxa_meta <- cbind(ctd_meta, conc_occ_final)
```


# # Check for unaccounted CPICS records 
```{r}
# Initialize a list to store unaccounted dates for each class
unaccounted_dates_list <- list()

# Iterate over each class object
for (class_name in class_names) {
  # Get the date-time objects from the current class
  sel_class_dates <- get(paste0("class.", class_name))$date
  
  # Initialize a logical vector to track whether each class date is accounted for
  is_accounted_for <- rep(FALSE, length(sel_class_dates))
  
  # Check each class date against the intervals in dt_list
  for (i in 1:length(dt_list)) {
    if (i < length(dt_list)) {
      # Check all intervals except the last one
      interval_start <- dt_list[i] - start
      interval_end <- dt_list[i + 1] - stop
    } else {
      # Last interval captures all data greater than or equal to the last dt_list
      interval_start <- dt_list[i] - start
      interval_end <- date_limit  # Use date limit above
    }
    
    # Mark class dates that fall within the current interval as accounted for
    is_accounted_for <- is_accounted_for | (sel_class_dates >= interval_start & sel_class_dates < interval_end)
  }
  
  # Subset the class dates that were not accounted for
  unaccounted_sel_class_dates <- sel_class_dates[!is_accounted_for]
  
  # Store the unaccounted dates in the list
  unaccounted_dates_list[[class_name]] <- unaccounted_sel_class_dates
}

# Print or view the unaccounted dates for each class
for (class_name in class_names) {
  cat("Unaccounted dates for class:", class_name, "\n")
  print(unaccounted_dates_list[[class_name]])
  cat("\n")
}
```

# # Calculate plankton concentration time series per station
```{r}
# Set sample_vol_per_min to a single value
taxa_meta$sampled_vol_per_min <- max(taxa_meta$sampled_vol_per_min, na.rm = TRUE)
# Set total_vol_sampled to common sample_vol_per_min 
taxa_meta <- taxa_meta %>%
  mutate(total_vol_sampled = sampled_vol_per_min * conc_deployDurMin)

# Calculate species concentration (counts/ml) for each species
taxa_concentration <- taxa_meta %>%
  mutate(across(c(Acantharea,
                  Centric,
                  Ceratium,
                  Chaetoceros,
                  Chaetognaths,
                  Chain2,
                  Chain3,
                  Ostracods,
                  Copepods,
                  Decapods,
                  Echinoderms,
                  Guinardia,
                  Jellies,
                  Larvaceans,
                  Neocalyptrella,
                  Noctiluca,
                  pellets,
                  Polychaets,
                  Pteropods, 
                  Tricho), ~ ./total_vol_sampled * 1e6)) %>%
  select(cruiseID,
         Station, 
         dec_lat, 
         dec_lon, 
         year, 
         month, 
         date, 
         temp..degC., 
         salinity,
         X8.day.seascapes,
         monthly.seascapes,
         total_vol_sampled,
         Acantharea,
         Centric,
         Ceratium,
         Chaetoceros,
         Chaetognaths,
         Chain2,
         Chain3,
         Ostracods,
         Copepods,
         Decapods,
         Echinoderms,
         Guinardia,
         Jellies,
         Larvaceans,
         Neocalyptrella,
         Noctiluca,
         pellets,
         Polychaets,
         Pteropods, 
         Tricho) %>%
  filter(!is.na(total_vol_sampled))

# # Rename column headers to fix taxonomy or variable naming
taxa_concentration <- taxa_concentration %>%
  rename(
    cruise_id = cruiseID,
    station = Station,
    decimalLatitude = dec_lat,
    decimalLongitude = dec_lon,
    year = year,
    month = month,
    collection_date = date,
    temp = temp..degC., 
    salinity = salinity,
    seascape_8day = X8.day.seascapes,
    seascape_monthly = monthly.seascapes,
    rhizaria = Acantharea,
    centric_diatoms = Centric,
    ceratium_spp = Ceratium,
    chaetoceros_spp = Chaetoceros,
    chaetognaths = Chaetognaths,
    mixed_chain_diatoms = Chain2,
    skeletonema_spp = Chain3,
    ostracoda = Ostracods,
    copepoda = Copepods,
    decapoda = Decapods,
    echinoderma = Echinoderms,
    g_striata = Guinardia,
    gelatinous = Jellies,
    appendicularia = Larvaceans,
    neocalyptrella_spp = Neocalyptrella,
    hemidiscus_spp = Noctiluca,
    fecal_pellets = pellets,
    polychaete = Polychaets,
    pteropoda = Pteropods, 
    trichodesmium_spp = Tricho 
  )
```

# # Add 'keyfield' term to 'taxa_concentration' using 'SFER_data'
```{r}
# --- Load SFER_data ---
# tryCatch is used to handle potential errors, like a missing file.
tryCatch({
  sfer_data <- read.csv("SFER_data.csv", stringsAsFactors = FALSE)
}, error = function(e) {
  stop("Error loading CSV files. Please ensure 'SFER_data.csv' and 'taxa_concentration.csv' are in your R working directory. Original error: ", e$message)
})

# --- Clean Inconsistent Data ---
# Standardize 'cruise_id' and 'station' values in both data frames
# to resolve known discrepancies before merging.

# Standardize 'cruise_id': H23138 -> HG23138
taxa_concentration$cruise_id[taxa_concentration$cruise_id == "H23138"] <- "HG23138"
# Standardize 'station' ID's
taxa_concentration$station[taxa_concentration$station == "21/LK"] <- "21LK"
taxa_concentration$station[taxa_concentration$station == "BG7B"] <- "BG7"
taxa_concentration$station[taxa_concentration$station == "BG15B"] <- "BG15"
taxa_concentration$station[taxa_concentration$station == "BG17B"] <- "BG17"

# Rename 'collection_date' column to 'datetime' in taxa_concentration
if ("collection_date" %in% names(taxa_concentration)) {
  names(taxa_concentration)[names(taxa_concentration) == "collection_date"] <- "datetime"
}

# Ensure 'datetime' columns are of the same type (character) for accurate merging
if ("datetime" %in% names(taxa_concentration) && "datetime" %in% names(sfer_data)) {
  taxa_concentration$datetime <- as.character(taxa_concentration$datetime)
  sfer_data$datetime <- as.character(sfer_data$datetime)
}

# Check if the 'depth_order' column (or other column as needed) exists in the sfer_data dataframe.
if (!"depth_class" %in% names(sfer_data)) {
  stop("The 'depth_class' column was not found in SFER_data.csv. The script cannot filter for 'Surface' values.")
}

# Use grepl() to find rows containing 'Surface' and subset the data frame.
sfer_data <- sfer_data[grepl("Surface", sfer_data$depth_class, ignore.case = TRUE), ]

# --- Prepare Data for Merging ---
# Create a subset of the SFER_data data frame.
# This new data frame contains only the columns needed for the join (the matching keys)
# and the 'keyfield' column to append.

key_columns <- c("cruise_id", "year", "month", "station", "datetime", "keyfield")

# Check if all necessary columns exist in SFER_data
if (!all(key_columns %in% names(sfer_data))) {
  stop("One or more required columns are missing from SFER_data.csv. Required columns are: ", paste(key_columns, collapse=", "))
}

sfer_subset <- sfer_data[, key_columns]

# --- Merge the Data Frames ---
# The merge() function performs a left join.
# - 'x = taxa_concentration': This is the left data frame. We want to keep all its rows.
# - 'y = sfer_subset': This is the right data frame, from which we'll pull 'keyfield'.
# - 'by = ...': A vector of column names to match rows on.
# - 'all.x = TRUE': This specifies the left join, ensuring all rows from taxa_concentration are kept.
#   If a row in taxa_concentration has no match in sfer_subset, the new 'keyfield' will be NA.
# - 'sort = FALSE': This preserves the original order of the taxa_concentration data frame as much as possible.

matching_keys <- c("cruise_id", "year", "month", "datetime", "station")
merged_data <- merge(taxa_concentration, sfer_subset, by = matching_keys, all.x = TRUE, sort = FALSE)

# --- Check for Unmatched Rows (NA in keyfield) ---
# Identify rows from 'taxa_concentration' that did not find a match in 'sfer_subset'.
# These rows will have an NA value in the 'keyfield' column.

unmatched_rows <- merged_data[is.na(merged_data$keyfield), ]

if (nrow(unmatched_rows) > 0) {
  cat("------------------------------------------------------------\n")
  cat("Found", nrow(unmatched_rows), "rows from 'taxa_concentration' that did not have a matching 'keyfield' in 'sfer_data'.\n\n")
  cat("The unmatched rows are:\n")
  print(unmatched_rows)
  cat("------------------------------------------------------------\n\n")
} else {
  cat("------------------------------------------------------------\n")
  cat("All rows in 'taxa_concentration' were successfully matched with a 'keyfield'.\n")
  cat("------------------------------------------------------------\n\n")
}

# --- Find and Report Duplicate Rows ---
# Identify rows in 'merged_data' that are duplicates based on the matching keys.
# This indicates that a single entry in 'taxa_concentration' matched multiple
# entries in 'sfer_data'.

is_duplicated <- duplicated(merged_data[, matching_keys]) | duplicated(merged_data[, matching_keys], fromLast = TRUE)
duplicate_rows <- merged_data[is_duplicated, ]

if (nrow(duplicate_rows) > 0) {
  cat("------------------------------------------------------------\n")
  cat("Found", nrow(duplicate_rows), "rows in 'merged_data' that are duplicates based on the keys:", paste(matching_keys, collapse=", "), ".\n")
  cat("This is caused by rows in 'taxa_concentration' matching multiple entries in 'sfer_data'.\n\n")
  cat("The duplicated rows are:\n")
  # Sort the duplicates so they appear next to each other for easy comparison
  sorted_duplicates <- duplicate_rows[order(duplicate_rows$cruise_id, duplicate_rows$year, duplicate_rows$month, duplicate_rows$station), ]
  print(sorted_duplicates)
  cat("------------------------------------------------------------\n\n")
}else {
  cat("------------------------------------------------------------\n")
  cat("No duplicate rows were found in 'merged_data'.\n")
  cat("------------------------------------------------------------\n\n")
}

# --- 8. Reorder Columns ---
# The user requested that 'keyfield' be the first column in the final table.
# We will create a new vector of column names in the desired order and
# use it to re-index the data frame.

# Get the name of all columns in the merged data frame
current_columns <- names(merged_data)

# Isolate the columns that are NOT 'keyfield'
other_columns <- current_columns[current_columns != "keyfield"]

# Create the new column order with 'keyfield' at the front
new_column_order <- c("keyfield", other_columns)

# Apply the new order to the data frame
final_taxa_data <- merged_data[, new_column_order]


# --- 9. Save the Result ---
# Save the newly created data frame to a new CSV file.
# Using a new file name prevents overwriting the original data.
# 'row.names = FALSE' prevents R from writing row numbers into the CSV.

output_filename <- "sfer_taxa_concentration.tsv"
write.table(final_taxa_data, output_filename, row.names = FALSE, sep = "\t")


# --- 10. Final Confirmation ---
# Print a message to the console confirming the script has finished
# and showing the first few rows of the new data.
cat("Script finished successfully!\n")
cat("The merged data has been saved to '", output_filename, "'\n\n")
cat("Preview of the final data:\n")
print(head(final_taxa_data))
```
